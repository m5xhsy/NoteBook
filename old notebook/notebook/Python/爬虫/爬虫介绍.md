# 爬虫

**通过编写程序，模拟浏览器上网，然后让其去互联网上爬取数据的过程**

## 爬虫的分类

- 通用爬虫
- 聚焦爬虫
- 增量式爬虫



## 解析

### 正则解析

```shell
单字符：
        . : 除换行以外所有字符
        [] ：[aoe] [a-w] 匹配集合中任意一个字符
        \d ：数字  [0-9]
        \D : 非数字
        \w ：数字、字母、下划线、中文
        \W : 非\w
        \s ：所有的空白字符包,括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。
        \S : 非空白
数量修饰：
        * : 任意多次  >=0
        + : 至少1次   >=1
        ? : 可有可无  0次或者1次
        {m} ：固定m次 hello{3,}
        {m,} ：至少m次
        {m,n} ：m-n次
边界：
        $ : 以某某结尾 
        ^ : 以某某开头
分组：
        (ab)  
贪婪模式： .*
非贪婪（惰性）模式： .*?

re.I : 忽略大小写
re.M ：多行匹配
re.S ：单行匹配

re.sub(正则表达式, 替换内容, 字符串)
```

### bs4解析

```shell
pip3 install lxml
pip3 install bs4
```

- **解析原理**
  1. 将要进行解析的源码加载到bs对象
  2. 调用bs对象中相关方法或者属性解析源码中的相关标签定位
  3. 将定位到的标签之间存在的文本或者属性值获取

- **使用方法**

  ```python
  from bs4 import BeautifulSoup
  
  soup = BeautifulSoup("xxx", "lxml")		# 字符串解析
  soup = BeautifulSoup(open("G:/1.html",mode="r",encoding="utf8"), "lxml") # 文件解析
  
  print(soup)						# 显示html中的内容
  
  # 根据标签名查找
  soup.a							# 只能找到匹配的第一个
  
  # 获取属性
  soup.a.attrs					# 获取a标签所有属性，返回一个字典
  soup.a.attrs["href"]			# 获取href属性(也可以写成soup.a["href"])
  
  # 获取标签内容
  soup.h1.string					# 如果标签中有标签，string中获取到的就是None
  soup.p.text
  soup.p.get_text()
  
  # 找到第一个符合要求的标签
  soup.find('a')  				# 找到第一个符合要求的
  soup.find('a', title="xxx")
  soup.find('a', alt="xxx")
  soup.find('a', class_="xxx")
  soup.find('a', id="xxx")
  
  # 找到所有符合要求的标签
  soup.find_all('a')
  soup.find_all(['a','b']) 		# 找到所有的a和b标签
  soup.find_all('a', limit=2) 	# 限制前两个
  
  # 根据选择器查找
  soup.select("#user")
  soup.select(".username > p")
  soup.select(".username, p")
  ```

### xpath解析

- **解析原理**
  1. 获取页面源码数据
  
  2. 实例化一个etree的对象，并且将页面源码数据加载到该对象中
  
3. 调用该对象的xpath方法进行指定标签的定位
  
  4. 注意: 
  
     1. xpath函数必须结合着xpath表达式解析标签定位和内容捕获
     2. xpath表达式中不能有tbody标签
  
     

- **语法**

  ```python
  /html/body/div/a/img				# html标签开始获取img标签
  //div[[@class="img_div"]]			# 获取class为img_div的标签
  //div/a/img/@src					# 获取img标签的src属性
  //div/p/text()						# 获取p标签的文本
  //div[@class="img_div"]//p/text()	# 获取clss为img_div的标签下所有p标签的文本
  //a[@href="" and @class="du"]       # 匹配href为“”且class为du 还可以用 | 或
  ```

- **示例**

  ```python
  from lxml import etree
  import requests
  
  url = 'http://pic.netbian.com/'
  headers = {
      "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36",
  }
  # html_text etree.parse("xxx.html")			# 从文本中获取html
  html_text = requests.get(url=url,headers=headers).text		# 获取html文本
  html_text = html_text.encode("iso-8859-1").decode("gbk")	# 是将gbk编码编码成unicode编码，再解码成gbk字符串
  tree = etree.HTML(html_text)								# 源码加载到etree中
  
  a_list = tree.xpath('//ul[@class="clearfix"]/li/a')			# 解析标签
  for a in a_list:
      print(f"{a.xpath('./b/text()')[0]} -- http://pic.netbian.com/{a.xpath('./span/img/@src')[0]}")
  ```

## Requests模块

```python
requests.get(url=url,proxies={"https":192.168.1.5:8888})    # IP代理



from requests import Session
session = Session()
t = session.get(url="https://www.baidu.com").text
print(t)
```



